# -*- coding: utf-8 -*-
"""LR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WXjV0AC8K7d4Mz8SeRowtTCrFov-XH6U

**Logistic Regression**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
import warnings
warnings.simplefilter('ignore')

"""**1. Data Exploration:**"""

train_data=pd.read_csv('https://raw.githubusercontent.com/ygtech27/Datasets/refs/heads/main/Titanic_train.csv')
train_data

train_data.info()

train_data.describe()

train_data.hist(bins=20,figsize=(9,7))
plt.show()

train_data.boxplot(figsize=(15,8))

sns.pairplot(train_data)

sns.set(font_scale=1)
sns.countplot(x='Sex',data=train_data,palette='plasma').set_title('how many men and women in traindata')

"""**2. Data Preprocessing:**"""

train_data.drop(['Embarked', 'Survived', 'Name', 'PassengerId','Cabin','Ticket'], axis=1, inplace=True)

train_data

train_data.isnull().sum()

#treaing null values
train_data['Age'].fillna(train_data['Age'].mean(),inplace=True)
train_data.isnull().sum()

train_data.info()

train_data=pd.get_dummies(train_data,columns=['Sex'],dtype=int)
train_data

test_data=pd.read_csv('https://raw.githubusercontent.com/ygtech27/Datasets/refs/heads/main/Titanic_test.csv')
test_data

test_data.info()
test_data.drop(['Embarked','Name', 'PassengerId','Cabin','Ticket'], axis=1, inplace=True)

test_data.isnull().sum()

test_data['Age'].fillna(test_data['Age'].mean(),inplace=True)
train_data.isnull().sum()

test_data=pd.get_dummies(test_data,columns=['Sex'],dtype=int)
test_data

test_data.hist(bins=20,figsize=(9,7))
plt.show()

target_train= train_data[['Survived']]
target_train

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
x_train,x_test,y_train,y_test=train_test_split(features_train,target_train,train_size=0.8,random_state=11)
logreg = LogisticRegression()
logreg.fit(x_train,y_train)

y_pred=logreg.predict(x_test)
y_pred

"""**4. Model Evaluation:**"""

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,classification_report,roc_auc_score,roc_curve
confusion_matrix(y_test,y_pred)

classification_report(y_test,y_pred)

print('ACCURACY:',accuracy_score(y_test,y_pred)*100)

roc_auc = roc_auc_score(y_test,y_pred)
print(roc_auc)

fpr,tpr,thr=roc_curve(y_test,y_pred)
plt.plot(fpr,tpr,label='logit model')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('ROC Curve')
plt.plot([0,1],[0,1],color='red',linestyle='--')
plt.grid(False)
plt.show()

"""**5. Interpretation:**"""

print("Intercept",logreg.intercept_)
print("slope",logreg.coef_)

import pickle
with open("logistic_model.pkl", "wb") as f:
    pickle.dump(logreg, f)



import pickle
import numpy as np

import pickle
import numpy as np
from sklearn.linear_model import LogisticRegression  # Include this to help pickle resolve class

# Load model
with open("logistic_model.pkl", "rb") as f:
    model = pickle.load(f)

"""**Interview Questions:**"""

#1. What is the difference between precision and recall?
#    Precision: Measures the accuracy of positive predictions. High precision means few false positives.
#    Recall: Measures how well the model identifies all actual positives. High recall means few false negatives.

#2. What is cross-validation, and why is it important in binary classification?
#    A technique to evaluate model performance by splitting data into multiple subsets (folds) and testing the model across these folds.
#    Important for reducing overfitting and ensuring the model generalizes well to new data.

pip install streamlit

